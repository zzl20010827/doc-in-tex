\documentclass[a4paper,oneside,12pt]{ctexart}
\usepackage{enumerate,setspace,geometry,graphicx,bm,mathrsfs,xcolor,varwidth,framed,amsfonts,amssymb,indentfirst,fancyhdr}
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage[thmmarks,hyperref]{ntheorem}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{cleveref}

\setlength{\headheight}{15pt}
\allowdisplaybreaks[4]
\onehalfspacing
\geometry{centering,left=2.54cm,right=2.54cm,top=3.18cm,bottom=3.18cm}
\pagestyle{fancy}
\fancyhead[L]{\kaishu 强基数学001}
\fancyhead[C]{\kaishu 张卓立}
\fancyhead[R]{\kaishu 学号:2204110786\ 序号:15}

{
    \theoremstyle{plain}
    \theoremheaderfont{\normalfont\bfseries}
    \theorembodyfont{\kaishu}
    \theoremseparator{.}
    \newtheorem{exercise}{习题}
}

{
    \theoremstyle{nonumberplain}
    \theoremheaderfont{\bfseries}
    \theorembodyfont{\normalfont}
    \theoremseparator{.}
    \newtheorem{solution}{解}
}

{
    \theoremstyle{nonumberplain}
    \theoremheaderfont{\bfseries}
    \theorembodyfont{\normalfont}
    \theoremsymbol{\ensuremath{\blacksquare}}
    \theoremseparator{.}
    \newtheorem{proof}{证明}
}

\crefname{exercise}{习题}{习题}
\crefname{figure}{图}{图}
\crefname{table}{表}{表}
\crefname{equation}{式}{式}
\creflabelformat{exercise}{(#2\theexercise#3)}

\newcommand{\dif}{\mathrm{d}}
\newcommand{\differ}{\backslash}
\newcommand{\ptl}{\partial}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\cR}{\mathcal{R}}
\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\bias}{\mathrm{Bias}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\poi}{\mathrm{Poi}}
\newcommand{\Beta}{\mathrm{Beta}}

\begin{document}
    \begin{center}
        \LARGE\bfseries
        数理统计作业
    \end{center}

    \begin{exercise}
        \label{ex:44}
        Let $X_1,\cdots,X_n$ be a random sample from 
        \begin{equation*}
            f(x;\theta)=e^{-(x-\theta)}I_{[\theta,\infty)}(x) \quad \text{for}\ -\infty<\theta<\infty.
        \end{equation*}

        \begin{enumerate}[($a$)]
            \item Find a sufficient statistic.
            \item Find a maximum-likelihood estimator of $\theta$.
            \item Find a method-of-moments estimator of $\theta$.   
            \item Is there a complete sufficient statistic? If so, find it.
            \item Find the UMVUE of theta if one exists.
            \item Find the Pitman estimator for the location parameter $\theta$.
            \item Using the prior density $g(\theta)=e^{-\theta}I_{(0,\infty)}(\theta)$, find the posterior Bayes estimator of $\theta$.
        \end{enumerate}
    \end{exercise}

    \begin{solution}
        ($a$) \begin{align*}
            L(\theta)&=e^{n\theta}e^{-\sum_{i=1}^nx_i}\prod_{i=1}^nI_{[\theta,\infty)}(x_i)\\
            &=e^{n\theta}e^{-\sum_{i=1}^nx_i}I_{(-\infty,y_1]}(x_i),
        \end{align*}
        $Y_1$是充分统计量. 

        ($b$) 当$x_i\geqslant \theta$时, $L(\theta)=e^{n\theta-\sum_{i=1}^nx_i}$, 则$\theta$的MLE是$\hat{\theta}_1=Y_1$.

        ($c$) 令\begin{align*}
            \bar{X}=\expect(X)&=\int_\theta^\infty e^{-(x-\theta)}x\dd{x}\\
            &=\theta+1,
        \end{align*}
        $\theta$的矩估计$\hat{\theta}_2=\bar{X}-1$.

        ($d$) 若对任意的统计量$z(Y_1),\expect[z(Y_1)]=0$, 即 
        \begin{equation*}
            \int_\theta ne^{n\theta-ny_1}z(y_1)\dd{y_1}=0,
        \end{equation*}
        关于$\theta$求导, 得
        \begin{equation*}
            -nz(\theta)+\int_\theta^\infty n^2e^{n\theta-ny_1}z(y_1)\dd{y_1}=0,
        \end{equation*}
        则对$\forall \theta, z(\theta)=0$, 那么$\prob(z(Y_1)=0)=1$, $Y_1$是完备充分统计量.

        ($e$) \begin{equation*}
            \expect(Y_1)=\int_0^\infty ne^{n\theta-ny_1}y_1\dd{y_1}=\frac{1}{n}+\theta,
        \end{equation*}
        $Y_1-\frac{1}{n}$是$\theta$的无偏估计, 那么$Y_1-\frac{1}{n}$是$\theta$的UMVUE.

        ($f$) $\theta$的Pitman估计是 
        \begin{align*}
            T_1(X_1,\cdots,X_n)&=\frac{\int \theta L(\theta)\dd{\theta}}{\int L(\theta)\dd{\theta}}\\
            &=\frac{\int_{-\infty}^{y_1}e^{n\theta-\sum_{i=1}^nX_i}\theta\dd{\theta}}{\int_{-\infty}^{y_1}e^{n\theta-\sum_{i=1}^nX_i}\dd{\theta}}\\
            &=Y_1-\frac{1}{n}.
        \end{align*}

        ($g$) $\theta$的Bayes后验估计是 
        \begin{align*}
            T_2(X_1,\cdots,X_n)&=\frac{\int\theta L(\theta)g(\theta)\dd{\theta}}{\int L(\theta)g(\theta)\dd{\theta}}\\
            &=\frac{\int_0^{y_1}e^{(n-1)\theta-\sum_{i=1}^nX_i}\theta\dd{\theta}}{\int_0^{y_1}e^{(n-1)\theta-\sum_{i=1}^nX_i}\dd{\theta}}\\
            &=\frac{((n-1)Y_1-1)e^{(n-1)Y_1}+1}{(n-1)(e^{(n-1)Y_1}-1)}.
        \end{align*}
    \end{solution}

    \begin{exercise}
        \label{ex:47}
        Let $X_1,\cdots,X_n$ be a random sample of size $n$ from the following discrete density: 
        \begin{equation*}
            f(x;\theta)=\begin{pmatrix}
                2\\
                x
            \end{pmatrix}\theta^x(1-\theta)^{2-x}I_{\{0,1,2\}}(x),
        \end{equation*}
        where $\theta>0$.

        \begin{enumerate}[($a$)]
            \item Is there a unidimensional sufficient statistic? If so, is it complete?
            \item Find a maximum-likelihood estimator of $\theta^2=\prob(X_l=2)$. Is it unbiased?
            \item Find an unbiased estimator of $\theta$ whose variance coincides with the corresponding Cram\'er-Rao lower bound if such 
            exists. If such an estimate does not exist, prove that it does not.
            \item Find a uniformly minimum-variance unbiased estimator of $\theta^2$ if such exists.
            \item Using the squared-error loss function find a Bayes estimator of $\theta$ with respect to the beta prior distribution \begin{equation*}
                g(\theta)=\frac{1}{\Beta(a,b)}\theta^{a-1}(1-\theta)^{b-1}I_{(0,1)}(\theta).
            \end{equation*}
            \item Using the squared-error loss function, find a minimax estimator of $\theta$.
            \item Find a mean-squared error consistent estimator of $\theta^2$.
        \end{enumerate}
    \end{exercise}

    \begin{solution}
        ($a$) \begin{equation*}
            f(x;\theta)=(1-\theta)^2I_{\{0,1,2\}}(x)\exp(x\log\frac{\theta}{1-\theta}),
        \end{equation*}
        则该分布属于指数族, 那么有完备充分统计量$\sum_{i=1}^nX_i$, 令$n=1$, 有完备充分统计量$X_1$.

        ($b$) \begin{align*}
            L(\theta)&=\prod_{i=1}^n\begin{pmatrix}
                2\\
                x_i
            \end{pmatrix}\theta^{x_i}(1-\theta)^{2-x_i}I_{\{0,1,2\}}(x_i)\\
            &\propto\theta^{\sum_{i=1}^nx_i}(1-\theta)^{2n-\sum_{i=1}^nx_i}.
        \end{align*}
        则令 
        \begin{equation*}
            \frac{\sum_{i=1}^nx_i}{\theta}+\frac{2n-\sum_{i=1}^nx_i}{1-\theta}=0,
        \end{equation*}
        得$\hat{\theta}=\frac{\sum_{i=1}^nx_i}{2n}$, 又$\theta>0$, 
        \begin{equation*}
            \widehat{\theta^2}=\frac{\left(\sum_{i=1}^nx_i\right)^2}{4n^2}.
        \end{equation*}
        因为$\expect\left(\sum_{i=1}^nX_i\right)^2=2n\theta+(4n^2-2n)\theta^2$, $\widehat{\theta^2}$不是无偏的.

        ($c$) 首先
        \begin{equation*}
            l(\theta)=\log\left[\prod_{i=1}^n\begin{pmatrix}
                2\\
                x_i
            \end{pmatrix}I_{\{0,1,2\}}(x_i)\right]+\left(\sum_{i=1}^nx_i\right)\log\theta+(2n-\sum_{i=1}^nx_i)\log(1-\theta),
        \end{equation*}
        则 
        \begin{equation*}
            \pdv{l}{\theta}=\frac{\sum_{i=1}^nx_i}{\theta}-\frac{2n-\sum_{i=1}^nx_i}{1-\theta}=\frac{\left(\sum_{i=1}^nx_i\right)/2n-\theta}{\theta(1-\theta)/2n}.
        \end{equation*}
        则$\frac{\sum_{i=1}^nX_i}{2n}$是达到C-R下界的无偏估计.

        ($d$) 因为$\expect\left(\sum_{i=1}^nX_i\right)=2n\theta,\expect\left(\sum_{i=1}^nX_i\right)^2=2n\theta+(4n^2-2n)\theta^2$, 那么 
        \begin{equation*}
            \expect\left(\frac{\left(\sum_{i=1}^nX_i\right)^2-\sum_{i=1}^nX_i}{4n^2-2n}\right)=\theta^2,
        \end{equation*}
        即$\frac{\left(\sum_{i=1}^nX_i\right)^2-\sum_{i=1}^nX_i}{4n^2-2n}$是$\theta^2$的一个无偏估计, 又因为$\sum_{i=1}^nX_i$是完备充分统计量, 
        则$\frac{\left(\sum_{i=1}^nX_i\right)^2-\sum_{i=1}^nX_i}{4n^2-2n}$是$\theta^2$是UMVUE.

        ($e$) 在均方误差损失函数下, $\theta$的Bayes估计是
        \begin{equation}
            \label{eq:ex47(e)的Bayes公式}
            \expect(\theta\mid X_1=x_1,\cdots,X_n=x_n)=\frac{\int_0^\infty \theta L(\theta)g(\theta)\dd{\theta}}{\int_0^\infty L(\theta)g(\theta)\dd{\theta}}.
        \end{equation}
        首先计算\cref{eq:ex47(e)的Bayes公式}的分子, 
        \begin{align*}
            \int_0^\infty \theta L(\theta)g(\theta)\dd{\theta}&=\prod_{i=1}^n\begin{pmatrix}
                2\\
                x_i
            \end{pmatrix}I_{\{0,1,2\}}(x_i)\int_0^\infty \theta\cdot\theta^{\sum_{i=1}^nx_i}(1-\theta)^{2n-\sum_{i=1}^nx_i}\\
            &\quad\cdot \frac{1}{\Beta(a,b)}\theta^{a-1}(1-\theta)^{b-1}I_{(0,1)}(\theta)\dd{\theta}\\
            &=\prod_{i=1}^n\begin{pmatrix}
                2\\
                x_i
            \end{pmatrix}I_{\{0,1,2\}}(x_i)\cdot\frac{1}{\Beta(a,b)}\int_0^1\theta^{a+\sum_{i=1}^nx_i+1-1}\\
            &\quad\cdot(1-\theta)^{2n-\sum_{i=1}^nx_i+b-1}\dd{\theta}\\
            &=\prod_{i=1}^n\begin{pmatrix}
                2\\
                x_i
            \end{pmatrix}I_{\{0,1,2\}}(x_i)\cdot\frac{\Beta\left(a+\sum_{i=1}^nx_i+1,2n-\sum_{i=1}^nx_i+b\right)}{\Beta(a,b)}.
        \end{align*}
        类似可得分母为 
        \begin{equation*}
            \int_0^\infty L(\theta)g(\theta)\dd{\theta}=\prod_{i=1}^n\begin{pmatrix}
                2\\
                x_i
            \end{pmatrix}I_{\{0,1,2\}}(x_i)\frac{\Beta\left(\sum_{i=1}^nx_i+a,2n-\sum_{i=1}^nx_i+b\right)}{\Beta(a,b)}.
        \end{equation*}
        代入到\cref{eq:ex47(e)的Bayes公式}, 得到Bayes估计为
        \begin{equation}
            \label{eq:ex47(e)Bayes估计}
            T(X_1,\cdots,X_n)=\frac{\Beta\left(a+\sum_{i=1}^nX_i+1,2n-\sum_{i=1}^nX_i+b\right)}{\Beta\left(\sum_{i=1}^nX_i+a,2n-\sum_{i=1}^nX_i+b\right)}.
        \end{equation}
        又因为 
        \begin{gather*}
            \Beta\left(a+\sum_{i=1}^nX_i+1,2n-\sum_{i=1}^nX_i+b\right)=\frac{\Gamma(a+\sum_{i=1}^nX_i+1)\Gamma(2n-\sum_{i=1}^nX_i+b)}{\Gamma(2n+a+b+1)},\\
            \Beta\left(\sum_{i=1}^nX_i+a,2n-\sum_{i=1}^nX_i+b\right)=\frac{\Gamma(\sum_{i=1}^nX_i+a)\Gamma(2n-\sum_{i=1}^nX_i+b)}{\Gamma(2n+a+b)},\\
            \Gamma\left(a+\sum_{i=1}^nX_i+1\right)=\left(a+\sum_{i=1}^nX_i\right)\Gamma\left(a+\sum_{i=1}^nX_i\right),\\
            \Gamma(2n+a+b+1)=(2n+a+b)\Gamma(2n+a+b),
        \end{gather*}
        将上式全部代入\cref{eq:ex47(e)Bayes估计}, 得到$\theta$的Bayes估计为 
        \begin{equation}
            \label{eq:ex47(e)答案}
            T(X_1,\cdots,X_n)=\frac{a+\sum_{i=1}^nX_i}{2n+a+b}.
        \end{equation}

        ($f$) 利用($e$)的结果, 其中$a,b$是待定的. 首先计算$\sum_{i=1}^nX_i$的分布, 因为对任意的$i$, $X_i$独立且$X_i\sim B(2,\theta)$, 那么$\sum_{i=1}^nX_i\sim B(2n,\theta)$, 根据\cref{eq:ex47(e)答案}, 
        \begin{align*}
            \cR_T(\theta)&=\expect[(T-\theta)^2]\\
            &=\frac{1}{(2n+a+b)^2}\expect\left(a-(a+b)\theta+\sum_{i=1}^nX_i-2n\theta\right)\\
            &=\frac{\Var\left(\sum_{i=1}^nX_i\right)+(a-(a+b)\theta)^2}{(2n+a^b)^2}\\
            &=\frac{2(n-a(a+b))\theta+((a+b)^2-2n)\theta^2+a^2}{(2n+a+b)^2},
        \end{align*}
        为了使上式等于常数, 令$(a+b)^2=2n,a(a+b)=n$, 即$a=b=\sqrt{\frac{n}{2}}$, 上式为常数, 即此时$T$是minimax估计.

        ($g$) 因为$X_i\sim B(2,\theta)$, $\expect(X_i)=2\theta,\expect(X_i^2)=2\theta+2\theta^2$, 则
        \begin{equation*}
            \expect\left(\frac{X_i^2-X_i}{2}\right)=\theta^2,
        \end{equation*}
        根据Khinchin大数定律, 
        \begin{equation*}
            \frac{\sum_{i=1}^nX_i^2-\sum_{i=1}^nX_i}{2n}\overset{p}{\longrightarrow} \theta^2,
        \end{equation*}
        即$\prob\left(\abs{\frac{\sum_{i=1}^nX_i^2-\sum_{i=1}^nX_i}{2n}-\theta^2}>\epsilon\right)\to 0, n\to\infty$, 又$0\leqslant \frac{\sum_{i=1}^nX_i^2-\sum_{i=1}^nX_i}{2n}\leqslant 1$, 
        则\begin{equation*}
            \lim_{n\to\infty}\expect\left(\frac{\sum_{i=1}^nX_i^2-\sum_{i=1}^nX_i}{2n}-\theta^2\right)^2=0,
        \end{equation*}
        即$\frac{\sum_{i=1}^nX_i^2-\sum_{i=1}^nX_i}{2n}$是$\theta^2$的均方相合估计量.
    \end{solution}
\end{document}